{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39578c1f",
   "metadata": {},
   "source": [
    "# Problem set 2 - Monte Carlo method & TD learning\n",
    "\n",
    "**Due: 11:59pm, October 10, 2025**\n",
    "\n",
    "### Problem 1 - Monte Carlo evaluation and Monte Carlo control (Coding).\n",
    "\n",
    "TODO list:\n",
    "\n",
    "- (1) Finish MC Policy Evaluation Code (10 pt)\n",
    "- (2) Try different step size and discuss (10 pt)\n",
    "- (3) Finish MC + Exploring Starts Code (10 pt)\n",
    "- (4) Plot the convergence behavior, discuss the plot (10 pt)\n",
    "- (5) Proof of Monte-Carlo control on *random walk* problem (20 pt)\n",
    "- (6) Finish MC + epsilon greedy Code (10 pt)\n",
    "\n",
    "\n",
    "<!-- - MC Policy Evaluation Code (long runtime)\n",
    "- MC Policy Evaluation Different Step size plot\n",
    "- MC + Exploring Starts Code (long run time)\n",
    "- MC + ES Observe Plot, describe observation\n",
    "- MC + ES proof \n",
    "- MC + epsilon greedy code  -->\n",
    "\n",
    "### Problem 2 - Temporal-difference evaluation, SARSA and Q-learning\n",
    "- (1) Finish TD evaluation Code (10 pt)\n",
    "- (2) Finish SARSA algorithm (10 pt)\n",
    "- (3) Finish Q-learning algorithm (10 pt)\n",
    "\n",
    "\n",
    "<!-- - TD Evaluation\n",
    "- SARSA\n",
    "- Q-Learning -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ba247a",
   "metadata": {},
   "source": [
    "## Problem 1: Monte-Carlo method on CliffWalk environment.\n",
    "\n",
    "Recall from last PSET, the **CliffWalking** gridworld is a 4×12 grid. The agent starts at the bottom-left cell and aims to reach the bottom-right. The bottom row between start and goal is a **cliff**; stepping into it ends the episode with a large penalty. Each non-terminal step yields −1; stepping into the cliff yields −100 (and termination).  \n",
    "\n",
    "In this problem we will **directly use Gym/Gymnasium** to interact with the environment (`CliffWalking-v1`) and perform **Monte-Carlo (MC) method**. Here we provide the print utility function as the same in PSET 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1b5d7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ----- Reproducibility -----\n",
    "seed = 0\n",
    "np.random.seed(seed)          # numpy RNG (env seeding can be done at reset time)\n",
    "\n",
    "# ----- Environment -----\n",
    "env = gym.make(\"CliffWalking-v1\")  # 4x12 grid; bottom row is cliff + goal\n",
    "\n",
    "# Pretty printing for small numeric tables\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "def print_values(values, nrow: int, ncol: int, title: str = \"State Values\"):\n",
    "    \"\"\"\n",
    "    Print a value table V in grid form.\n",
    "    \"\"\"\n",
    "    values = np.asarray(values).reshape(nrow, ncol)\n",
    "    print(title)\n",
    "    for r in range(nrow):\n",
    "        print(\" \".join(f\"{values[r, c]:6.2f}\" for c in range(ncol)))\n",
    "    print()\n",
    "\n",
    "def print_policy(pi, nrow: int, ncol: int, title: str = \"Policy\"):\n",
    "    \"\"\"\n",
    "    Render a policy on the CliffWalking grid as arrows.\n",
    "\n",
    "    The environment uses the action indexing:\n",
    "        0 = UP, 1 = RIGHT, 2 = DOWN, 3 = LEFT\n",
    "    \"\"\"\n",
    "    arrow = {0: \"^\", 1: \">\", 2: \"v\", 3: \"<\"}  # matches env action semantics\n",
    "    print(title)\n",
    "    for i in range(nrow):\n",
    "        row_syms = []\n",
    "        for j in range(ncol):\n",
    "            s = i * ncol + j\n",
    "            p = pi[s]\n",
    "\n",
    "            # Determine greedy action(s)\n",
    "            if isinstance(p, list) and len(p) == 4:\n",
    "                best = np.argwhere(np.array(p) == np.max(p)).flatten().tolist()\n",
    "            elif isinstance(p, int):\n",
    "                best = [p]\n",
    "            else:\n",
    "                arr = np.array(p, dtype=float).ravel()\n",
    "                best = np.argwhere(arr == np.max(arr)).flatten().tolist()\n",
    "\n",
    "            # Cliff/goal cells (bottom row except column 0) rendered as terminal\n",
    "            if i == nrow - 1 and j > 0:\n",
    "                row_syms.append(\"T\")\n",
    "            else:\n",
    "                row_syms.append(\"\".join(arrow[a] for a in best))\n",
    "        print(\" \".join(sym if sym else \".\" for sym in row_syms))\n",
    "    print()\n",
    "\n",
    "# ----- Reference table (ground-truth under random policy from PSET1) -----\n",
    "# Shape is (4, 12) in row-major order, then flattened to 1D for convenience.\n",
    "# Values correspond to the state-value function V^π for the UNIFORM RANDOM policy.\n",
    "V_random_gt = np.array([\n",
    "    [-143.21, -147.36, -151.35, -153.93, -155.11, -155.05, -153.66, -150.44, -144.43, -134.39, -119.88, -105.06],\n",
    "    [-164.99, -174.34, -180.41, -183.52, -184.80, -184.82, -183.62, -180.68, -174.67, -162.95, -141.20, -108.16],\n",
    "    [-207.96, -237.09, -246.20, -249.36, -250.43, -250.52, -249.79, -247.81, -243.17, -231.62, -199.35,  -96.76],\n",
    "    [-261.35,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00,    0.00],\n",
    "], dtype=float).flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2d2dcd",
   "metadata": {},
   "source": [
    "### 1.1 Monte-Carlo evaluation\n",
    "\n",
    "Let $\\mathcal{D}(s)$ denote the set of all time indices at which state $s$ is visited across sampled episodes. Then the Monte Carlo estimate of the value function is\n",
    "$$\n",
    "\\hat{V}(s)=\\frac{1}{|\\mathcal{D}(s)|}\\sum_{t\\in \\mathcal{D}(s)} g_t .\n",
    "\\tag{2.3}\n",
    "$$\n",
    "\n",
    "There are two common variants:\n",
    "\n",
    "- **First-visit MC:** use only the first occurrence of $s$ in each episode.\n",
    "- **Every-visit MC:** use all occurrences of $s$ within an episode.\n",
    "\n",
    "You can test both of them in 1.2. For 1.1 you need to finish the **first-visit** one.\n",
    "\n",
    "**TODO: Finish the code block for monte-carlo evaluation.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_evaluate(policy, env, episodes=5000, gamma=0.95, seed=None):\n",
    "    \"\"\"\n",
    "    First-visit Monte Carlo (MC) state-value prediction (Sutton & Barto, Alg. 5.1).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy : ndarray, shape (nS, nA)\n",
    "        Row-stochastic policy: for each state s, policy[s] is a prob. dist. over actions.\n",
    "    env : Gymnasium-like environment\n",
    "        Must expose discrete observation_space.n and action_space.n and return\n",
    "        (obs, reward, terminated, truncated, info) from step().\n",
    "    episodes : int\n",
    "        Number of episodes to sample.\n",
    "    gamma : float\n",
    "        Discount factor in [0, 1].\n",
    "    seed : int | None\n",
    "        If given, used to seed a NumPy RNG and (re)seed env at each episode start.\n",
    "    \"\"\"\n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "    V = np.zeros(nS, dtype=float)          # value estimates\n",
    "    N_first = np.zeros(nS, dtype=int)      # first-visit counts per state\n",
    "    visits = np.zeros(nS, dtype=int)       # total visits (diagnostic only)\n",
    "    errors = []\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    for ep in tqdm(range(episodes), desc=\"MC first-visit\"):\n",
    "        # Episode generation under π\n",
    "        s, _ = env.reset(seed=int(rng.integers(1e9)) if seed is not None else None)\n",
    "        states, rewards = [], []\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            ##########################################\n",
    "            # TODO: sample action from policy and step in env\n",
    "            # hint: use env.step(a) to get (s', r, terminated, truncated, info)\n",
    "            ##########################################\n",
    "\n",
    "        # Identify first visits of each state in this episode\n",
    "        first_visit_mask = [False] * len(states)\n",
    "        seen_from_start = set()\n",
    "        for t, st in enumerate(states):\n",
    "            if st not in seen_from_start:\n",
    "                seen_from_start.add(st)\n",
    "                first_visit_mask[t] = True\n",
    "\n",
    "        # Backward return accumulation; update ONLY on first visits\n",
    "        ##########################################\n",
    "        # TODO: update V and N_first for first-visit states\n",
    "        ##########################################\n",
    "\n",
    "        # Track RMSE to a reference table for the random policy (from PSET1)\n",
    "        rmse = float(np.sqrt(np.mean((V - V_random_gt) ** 2)))\n",
    "        errors.append(rmse)\n",
    "\n",
    "    return V, errors\n",
    "\n",
    "\n",
    "# Uniform-random policy for evaluation\n",
    "policy = np.ones((env.observation_space.n, env.action_space.n)) / env.action_space.n\n",
    "\n",
    "V, errors = mc_evaluate(policy, env, episodes=1000)\n",
    "\n",
    "print_values(V, 4, 12, \"Estimated State Values (MC First-Visit)\")\n",
    "# Compare visually with PSET1 reference; they should be close (up to sampling noise).\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(errors)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"RMSE vs V_random_gt\")\n",
    "plt.title(\"MC Evaluation RMSE\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Note: On my laptop this may take ~1 minute for 1k episodes, depending on Python + Gym versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec8a90f",
   "metadata": {},
   "source": [
    "### 1.2 Plot for different step size.\n",
    "\n",
    "Recall equation (2.4) in the lecture note:\n",
    "$$\n",
    "\\hat{V}(s) \\leftarrow \\hat{V}(s) + \\alpha_{N(s)}\\!\\left(g_t - \\hat{V}(s)\\right),\n",
    "\\qquad \\alpha_{N(s)} > 0 \\text{ diminishing.}\n",
    "$$\n",
    "\n",
    "For standard Monte-Carlo evaluation, you will take the $\\alpha_{N(s)}$ as $\\frac{1}{N(s)}$. But you have more options: you can do constant step or other steps you prefer.\n",
    "\n",
    "**TODO: test different step size options (sample average, constant step size with different values, and another step size schedule that satisfies Robbins-Monro), and you can also change first-visit to every-visit. What do you observe?**\n",
    "\n",
    "Hint: you can try save different loss curves and plot in the same plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16665efa",
   "metadata": {},
   "source": [
    "### 1.3 Monte-Carlo control with Exploring Starts\n",
    "\n",
    "In the class we talk about two methods to ensure exploration: exploring starts and $\\epsilon$-greedy policy. We will start with exploring starts for monte-carlo control.\n",
    "\n",
    "**TODO: finish code of Monte-Carlo control with exploring starts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_state(env, s):\n",
    "    \"\"\"Attempt to set the environment's internal discrete state to `s`\n",
    "    (needed for Exploring Starts).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Many classic-control/grid envs expose `unwrapped.s`\n",
    "        env.unwrapped.s = s\n",
    "        return True\n",
    "    except Exception:\n",
    "        # Some variants use `.state` instead\n",
    "        try:\n",
    "            env.unwrapped.state = s\n",
    "            return True\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "\n",
    "def mc_control_exploring_starts(env, episodes=5000, gamma=0.95, max_steps=500, alpha=None):\n",
    "    \"\"\"\n",
    "    Monte Carlo control with Exploring Starts (ES).  See Sutton & Barto, Alg. 5.3.\n",
    "\n",
    "    Procedure\n",
    "    ---------\n",
    "    1) For each episode, sample an initial state–action pair (S0, A0) at random so that\n",
    "       every (s, a) has nonzero probability.\n",
    "    2) Roll out the *entire* episode under the current policy π (use A0 at t=0; thereafter\n",
    "       sample actions from π).\n",
    "    3) At the end of the episode, perform **first-visit** MC updates to Q(s, a) using the\n",
    "       return G from each first-visited (s, a).\n",
    "    4) For all states visited in this episode, improve the policy greedily:\n",
    "       π(s) ← argmax_a Q(s, a) (break ties uniformly).\n",
    "    \"\"\"\n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "    Q = np.zeros((nS, nA), dtype=float)\n",
    "    N_visits = np.zeros((nS, nA), dtype=int)\n",
    "    V_snaps = []\n",
    "\n",
    "    # Initialize π arbitrarily: start with a uniform distribution over actions.\n",
    "    policy = np.ones((nS, nA), dtype=float) / nA\n",
    "\n",
    "    for ep in tqdm(range(episodes), desc=\"MC ES (Exploring Starts)\"):\n",
    "        # --- Exploring Start: choose (S0, A0) uniformly at random ---\n",
    "        # For CliffWalking we avoid terminal/cliff cells by excluding the last row\n",
    "        # except the start (index 36). `nS - 11` ensures s0 ∈ {0..36}.\n",
    "        s0 = np.random.randint(nS - 11)\n",
    "        a0 = np.random.randint(nA)\n",
    "\n",
    "        # Reset env, then force the start state to s0. If not supported, ES cannot be run strictly.\n",
    "        s, _ = env.reset()\n",
    "        if not _set_state(env, s0):\n",
    "            raise RuntimeError(\n",
    "                \"Environment does not support setting arbitrary start states; \"\n",
    "                \"strict Exploring Starts is not possible.\"\n",
    "            )\n",
    "\n",
    "        # --- Generate a full trajectory under current π (A0 at t=0, then follow π) ---\n",
    "        states, actions, rewards = [], [], []\n",
    "        s = s0\n",
    "        a = a0\n",
    "        done = False\n",
    "\n",
    "        # Cap length to avoid pathological loops when π does not reach a terminal.\n",
    "        for _ in range(max_steps):\n",
    "            ##########################################\n",
    "            # TODO: sample action from policy and step in env\n",
    "            # hint: use env.step(a) to get (s', r, terminated, truncated, info)\n",
    "            ##########################################\n",
    "\n",
    "        # --- First-visit masks for (s, a) within this episode ---\n",
    "        first_visit_sa_mask = [False] * len(states)\n",
    "        seen_from_start_sa = set()\n",
    "        for t, (st, at) in enumerate(zip(states, actions)):\n",
    "            if (st, at) not in seen_from_start_sa:\n",
    "                seen_from_start_sa.add((st, at))\n",
    "                first_visit_sa_mask[t] = True\n",
    "\n",
    "        # --- Backward return accumulation; update Q only at first visits ---\n",
    "        ##########################################\n",
    "        # TODO: update Q and N_first for first-visit states\n",
    "        ##########################################\n",
    "\n",
    "        # --- Greedy policy improvement on states encountered this episode (uniform tie-breaking) ---\n",
    "        ##########################################\n",
    "        # TODO: update policy to be greedy wrt. Q for states visited in this episode\n",
    "        ##########################################\n",
    "\n",
    "        # Keep a few early snapshots of V(s) for quick diagnostics\n",
    "        if (ep % 100) == 0 and ep <= 10000:\n",
    "            V = Q.max(axis=1)\n",
    "            V_snaps.append(V.copy())\n",
    "\n",
    "    return policy, Q, V_snaps\n",
    "\n",
    "\n",
    "policy_es, Q_es, V_snaps = mc_control_exploring_starts(env, episodes=200000, gamma=0.95)\n",
    "\n",
    "V_es = (policy_es * Q_es).sum(axis=1)\n",
    "print_values(V_es, 4, 12, \"V from MC-ES policy\")\n",
    "print_policy(policy_es, 4, 12, \"MC-ES derived policy\")\n",
    "\n",
    "# Note: With 200k episodes this can take around a minute on a typical laptop\n",
    "# (depends on Python/Gym versions and hardware).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583264b",
   "metadata": {},
   "source": [
    "### 1.4 Convergence behavior\n",
    "\n",
    "Use the `V_snap` you get from Monte-Carlo control to plot the convergence evolution.\n",
    "\n",
    "**TODO: what do you observe form the plot?**\n",
    "\n",
    "Hint: which part converge first?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d461688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Basic check\n",
    "V_snaps = [np.asarray(v).ravel() for v in V_snaps]\n",
    "K = int(V_snaps[0].size)\n",
    "if K != 48:\n",
    "    raise ValueError(f\"Expected V.size == 48 to reshape into 4x12, but got {K}.\")\n",
    "\n",
    "# Fixed color scale to avoid per-frame jumps (do not specify a colormap; use defaults)\n",
    "V_all = np.vstack(V_snaps)\n",
    "vmin, vmax = -20, 0\n",
    "\n",
    "# Initialize figure: 2D grid\n",
    "fig, ax = plt.subplots()\n",
    "grid0 = V_snaps[0].reshape(4, 12)\n",
    "im = ax.imshow(grid0, vmin=vmin, vmax=vmax)  # Do not specify cmap\n",
    "ax.set_xlabel(\"col\")\n",
    "ax.set_ylabel(\"row\")\n",
    "\n",
    "def init():\n",
    "    im.set_data(grid0)\n",
    "    return (im,)  # blit requires a tuple\n",
    "\n",
    "def update(i):\n",
    "    grid = V_snaps[i].reshape(4, 12)\n",
    "    im.set_data(grid)\n",
    "    return (im,)\n",
    "\n",
    "ani = animation.FuncAnimation(\n",
    "    fig, update, frames=len(V_snaps), init_func=init, blit=True, interval=400, repeat=False\n",
    ")\n",
    "\n",
    "gif_path = \"./mc_es_grid_evolution.gif\"\n",
    "ani.save(gif_path, writer=\"pillow\")\n",
    "plt.close(fig)\n",
    "\n",
    "display(Image(filename=gif_path))\n",
    "print(\"Saved GIF to:\", gif_path)\n",
    "\n",
    "# Print the last frame as a 4x12 grid\n",
    "V_last = V_snaps[-1].reshape(4, 12)\n",
    "np.set_printoptions(precision=2, suppress=True)  # Pretty printing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4110acb",
   "metadata": {},
   "source": [
    "### 1.5 Random walk on a ring: Why can monte-carlo method works? (Pen and Paper)\n",
    "\n",
    "In this problem you will use a random walk problem to analyze the convergence behavior of Monte-Carlo control.\n",
    "\n",
    "Consider a 1-D random walk problem on a ring, and one of them is the terminal state. Assume states are $\\{0,1,2,...,K - 1\\}$, and $0$ is the terminal state. The transition model can be written as $x_{k+1} = \\text{mod}(x_k + u_k)$ where $u_k = -1 / 1$.\n",
    "\n",
    "**Setup.**  \n",
    "States $ \\mathcal S=\\{0,1,\\dots,K-1\\} $ with terminal $0$ (absorbing).  \n",
    "Actions $ \\mathcal A=\\{-1,+1\\} $, transition $ s'=(s+a)\\bmod K $ (Note that terminal is absorb, so once it arrive $0$ it will not move).  \n",
    "Per-step reward $-1$ until termination; discount $ \\gamma\\in(0,1] $.  \n",
    "Distance to $0$: $ d(s)=\\min\\{s,K-s\\} $.  \n",
    "Algorithm: first-visit Monte-Carlo control with **Exploring Starts**.\n",
    "\n",
    "**Claim.** With probability $1$, the learned policy converges to\n",
    "$$\n",
    "\\pi^\\star(s)=\n",
    "\\begin{cases}\n",
    "\\text{move to reduce } d(s), & d(s)\\neq K/2,\\\\\n",
    "\\text{either action}, & d(s)=K/2 \\text{ (if $K$ even),}\n",
    "\\end{cases}\n",
    "$$\n",
    "i.e., “go along the shortest arc to $0$”.\n",
    "\n",
    "### Proof (outward induction on $d$)\n",
    "\n",
    "First we write down the optimal value $V^\\star$ under $\\pi^\\star$\n",
    "\n",
    "**(a) TODO: write down $V^\\star$** \n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**Base of induction ($d=0$).** \n",
    "\n",
    "**(b) TODO: Show that $Q(0,a)\\equiv 0$** \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "**Inductive hypothesis.** For every state with $d\\le m-1$, the maximization of $Q(s,a)$ is already optimal, i.e.\n",
    "$$\n",
    "\\max_a Q(s,a) := V(s) = V^\\star\n",
    "$$\n",
    "\n",
    "**(c) TODO: For $d(s)=m$, show that $Q(s,\\text{toward}) \\leq Q(s,\\text{away})$ (equality hold only when $s = \\frac{K}{2}$)** \n",
    "\n",
    "**Answer:** \n",
    "\n",
    "//\n",
    "\n",
    "Therefore greedy improvement sets $\\pi(s)=\\text{toward}$. By convergence of monte-carlo evaluation under fix policy, $Q(s,\\text{toward})$ finally converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb3629b",
   "metadata": {},
   "source": [
    "### 1.6 Monte-Carlo control with $\\epsilon$-soft policies\n",
    "\n",
    "**TODO: finish the code for Monte-Carlo control with $\\epsilon$-soft policies** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97ba63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _epsilon_soft_from_Q(Q_row: np.ndarray, epsilon: float, tol: float = 1e-12) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build an ε-soft *greedy* action distribution from a vector of Q-values.\n",
    "\n",
    "    Rule:\n",
    "      1) Give every action a base mass ε/|A|.\n",
    "      2) Split the remaining (1-ε) uniformly among the greedy (argmax) actions.\n",
    "         (Deterministic tie-handling: no random tie-breaking.)\n",
    "    \"\"\"\n",
    "    nA = Q_row.shape[0]\n",
    "    probs = np.full(nA, epsilon / nA, dtype=float)\n",
    "\n",
    "    m = Q_row.max()\n",
    "    ties = np.flatnonzero(np.isclose(Q_row, m, atol=tol, rtol=0.0))\n",
    "    share = (1.0 - epsilon) / ties.size\n",
    "    probs[ties] += share\n",
    "    return probs\n",
    "\n",
    "\n",
    "def mc_control_onpolicy_epsilon_soft(\n",
    "    env,\n",
    "    episodes: int = 5000,\n",
    "    gamma: float = 0.95,\n",
    "    max_steps: int = 500,\n",
    "    alpha: float | None = None,   # None → sample mean 1/N(s,a); else constant step-size\n",
    "    epsilon: float = 0.1,         # constant ε if no schedule is provided\n",
    "    eps_schedule=None,            # optional callable: eps = eps_schedule(ep)  (GLIE-style)\n",
    "    min_epsilon: float = 0.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    On-policy first-visit Monte Carlo control with ε-soft policies\n",
    "    (Sutton & Barto, On-policy MC Control).\n",
    "\n",
    "    Key differences from MC with Exploring Starts (ES):\n",
    "      • No exploring starts; episodes are generated by the current ε-soft policy π.\n",
    "      • Policy improvement *keeps* π ε-soft: greedy actions get (1-ε)+ε/|A|, others ε/|A|.\n",
    "    \"\"\"\n",
    "    nS = env.observation_space.n\n",
    "    nA = env.action_space.n\n",
    "\n",
    "    Q = np.zeros((nS, nA), dtype=float)\n",
    "    N_first = np.zeros((nS, nA), dtype=int)\n",
    "\n",
    "    # Initialize π arbitrarily: uniform over actions (already ε-soft for any ε).\n",
    "    policy = np.full((nS, nA), 1.0 / nA, dtype=float)\n",
    "\n",
    "    ep_idx, V_snaps = [], []\n",
    "\n",
    "    for ep in tqdm(range(episodes), desc=\"MC (ε-soft)\"):\n",
    "        # GLIE-style ε schedule (optional)\n",
    "        eps = float(eps_schedule(ep)) if eps_schedule is not None else float(epsilon)\n",
    "        eps = float(np.clip(eps, min_epsilon, 1.0))\n",
    "\n",
    "        # ---- Generate one episode under current ε-soft policy π ----\n",
    "        states, actions, rewards = [], [], []\n",
    "        s, _ = env.reset()\n",
    "        a = int(np.random.choice(nA, p=policy[s]))  # first action from current policy\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            ##########################################\n",
    "            # TODO: sample action from policy and step in env\n",
    "            # hint: use env.step(a) to get (s', r, terminated, truncated, info)\n",
    "            ##########################################\n",
    "\n",
    "        # ---- First-visit markers for (s,a) within this episode ----\n",
    "        seen = set()\n",
    "        first_visit = [False] * len(states)\n",
    "        for t, (st, at) in enumerate(zip(states, actions)):\n",
    "            if (st, at) not in seen:\n",
    "                seen.add((st, at))\n",
    "                first_visit[t] = True\n",
    "\n",
    "        # ---- Backward return accumulation; update Q only at first visits ----\n",
    "        ##########################################\n",
    "        # TODO: update Q and N_first for first-visit states\n",
    "        ##########################################\n",
    "\n",
    "        # ---- ε-soft greedy policy improvement (only on states seen this episode) ----\n",
    "        visited_states = set(states)\n",
    "        for st in visited_states:\n",
    "            policy[st] = _epsilon_soft_from_Q(Q[st], eps)\n",
    "\n",
    "        # Light snapshots for convergence plots (every 100 eps)\n",
    "        if (ep % 100) == 0:\n",
    "            V = Q.max(axis=1)\n",
    "            V_snaps.append(V.copy())\n",
    "\n",
    "    return policy, Q, V_snaps\n",
    "\n",
    "\n",
    "def eps_schedule(ep, c=2000.0, eps_min=0.01):\n",
    "    \"\"\"Simple GLIE schedule: ε_ep = max(eps_min, c / (c + ep)).\"\"\"\n",
    "    return max(eps_min, c / (c + ep))\n",
    "\n",
    "\n",
    "policy, Q, V_snaps = mc_control_onpolicy_epsilon_soft(\n",
    "    env,\n",
    "    episodes=10000,\n",
    "    gamma=0.95,\n",
    "    epsilon=1.0,  # start fully exploratory; schedule will decay it\n",
    "    eps_schedule=lambda ep: eps_schedule(ep, c=2000.0, eps_min=0.01),\n",
    "    min_epsilon=0.0,\n",
    "    alpha=0.01    # constant step-size for Q-updates (try None for 1/N(s,a))\n",
    ")\n",
    "\n",
    "V_soft = (policy * Q).sum(axis=1)\n",
    "print_values(V_soft, 4, 12, \"V from MC ε-soft policy\")\n",
    "print_policy(policy, 4, 12, \"MC ε-soft derived policy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac2f8d6",
   "metadata": {},
   "source": [
    "## Problem 2: SARSA & Q-learning on Cliffwalk environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ce5039",
   "metadata": {},
   "source": [
    "### 2.1 TD evaluation\n",
    "\n",
    "**Algorithmic Form.** Suppose the agent is in state $s_t$, takes action $a_t \\sim \\pi(\\cdot \\mid s_t)$, receives reward $r_t$, and transitions to $s_{t+1}$. The TD(0) update rule is\n",
    "$$\n",
    "\\hat V(s_t) \\leftarrow \\hat V(s_t) + \\alpha\\,[\\,r_t + \\gamma \\hat V(s_{t+1}) - \\hat V(s_t)\\,],\n",
    "\\tag{2.6}\n",
    "$$\n",
    "where $\\alpha \\in (0,1]$ is the learning rate.\n",
    "\n",
    "The term inside the brackets,\n",
    "$$\n",
    "\\delta_t = r_t + \\gamma \\hat V(s_{t+1}) - \\hat V(s_t),\n",
    "\\tag{2.7}\n",
    "$$\n",
    "is called the **TD error**.\n",
    "\n",
    "**TODO: finish the code for TD evluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def90b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def td0_evaluate(env, episodes=5000, gamma=0.95, alpha=0.1, seed=None):\n",
    "    \"\"\"\n",
    "    On-policy temporal-difference evaluation under a **uniform random policy**.\n",
    "\n",
    "    Computes:\n",
    "      • V_TD0 : TD(0) / 1-step bootstrap   V(s_t) ← V(s_t) + α [ r_t + γ V(s_{t+1}) − V(s_t) ]\n",
    "      • V_TD2 : 2-step TD for the *previous* state s_{t-1}\n",
    "                V(s_{t-1}) ← V(s_{t-1}) + α [ r_{t-1} + γ r_t + γ^2 V(s_{t+1}) − V(s_{t-1}) ]\n",
    "                (This is the n=2 special case of n-step TD.)\n",
    "    Returns V_TD0 and RMSE traces for both estimators vs the provided V_random_gt.\n",
    "    \"\"\"\n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "    policy = np.ones((nS, nA)) / nA\n",
    "\n",
    "    V_TD0 = np.zeros(nS, dtype=float)\n",
    "    V_TD2 = np.zeros(nS, dtype=float)\n",
    "\n",
    "    visit_counts = np.zeros(nS, dtype=int)  # diagnostic only\n",
    "    errors_1, errors_2 = [], []\n",
    "\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    for _ in tqdm(range(episodes), desc=\"TD evaluation\"):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "\n",
    "        # Keep the previous transition to form a 2-step target\n",
    "        prev_state = None      # will hold s_{t-1}\n",
    "        prev_reward = None     # will hold r_{t-1}\n",
    "\n",
    "        while not done:\n",
    "            ##########################################\n",
    "            # TODO: step the environment, update Q\n",
    "            ##########################################\n",
    "\n",
    "        # Episode-level RMSE diagnostics\n",
    "        errors_1.append(np.sqrt(np.mean((V_TD0 - V_random_gt) ** 2)))\n",
    "        errors_2.append(np.sqrt(np.mean((V_TD2 - V_random_gt) ** 2)))\n",
    "\n",
    "    return V_TD0, (errors_1, errors_2)\n",
    "\n",
    "\n",
    "# Example run\n",
    "V, (errors_1, errors_2) = td0_evaluate(env, episodes=1000, alpha=0.01)\n",
    "\n",
    "print_values(V, 4, 12, \"Estimated State Values (TD(0) evaluation)\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(errors_1, label=\"TD(0)\")\n",
    "plt.plot(errors_2, label=\"2-step TD\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"RMSE vs V_random_gt\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Note: On my laptop this may take ~20s for 1k episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a07afe4",
   "metadata": {},
   "source": [
    "### 2.2 SARSA algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(\n",
    "    env,\n",
    "    episodes=5000,\n",
    "    Q_init=None,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.01,\n",
    "    alpha=0.1,\n",
    "    max_steps=500,       # per-episode safety cap\n",
    "):\n",
    "    \"\"\"\n",
    "    On-policy SARSA(0) control for **tabular, discrete** Gymnasium environments.\n",
    "\n",
    "    Behavior policy: ε-greedy w.r.t. current Q.\n",
    "    Update rule (per step):\n",
    "        Q(s,a) ← Q(s,a) + α [ r + γ Q(s', a') − Q(s,a) ],\n",
    "    where a' is sampled ε-greedily from state s' (on-policy).\n",
    "    \"\"\"\n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "    Q = Q_init.copy() if Q_init is not None else np.zeros((nS, nA), dtype=float)\n",
    "    visits = np.zeros((nS, nA), dtype=int)\n",
    "    V_snaps = []\n",
    "\n",
    "    def eps_greedy_action(s, eps):\n",
    "        \"\"\"ε-greedy over Q[s]: uniform random with prob ε; otherwise greedy with random tie-break.\"\"\"\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.randint(nA)\n",
    "        row = Q[s]\n",
    "        m = row.max()\n",
    "        candidates = np.flatnonzero(row == m)\n",
    "        return int(np.random.choice(candidates))\n",
    "\n",
    "    for ep in tqdm(range(episodes), desc=\"SARSA control\"):\n",
    "        s, _ = env.reset()\n",
    "        a = eps_greedy_action(s, epsilon)\n",
    "\n",
    "        # (Optional) record of visited states; useful for debugging/plots.\n",
    "        s_record = []\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            ##########################################\n",
    "            # TODO: step the environment, update Q and choose action\n",
    "            ##########################################\n",
    "\n",
    "        # Lightweight snapshots early on (every 2 episodes up to 100)\n",
    "        if (ep % 2) == 0 and ep <= 100:\n",
    "            V_snaps.append(Q.max(axis=1).copy())\n",
    "\n",
    "    return Q, visits, V_snaps\n",
    "\n",
    "\n",
    "Q, visits, V_snaps = sarsa(env, episodes=50000, gamma=0.95, alpha=0.01, epsilon=0.01)\n",
    "V = Q.max(axis=1)\n",
    "print_values(V, 4, 12, title=\"State Values (greedy after SARSA)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aab939a",
   "metadata": {},
   "source": [
    "### 2.3 Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3efe2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(\n",
    "    env,\n",
    "    episodes=20000,\n",
    "    gamma=0.95,\n",
    "    epsilon=0.01,\n",
    "    alpha=0.1,\n",
    "    max_steps=500,       # per-episode safety cap\n",
    "):\n",
    "    \"\"\"\n",
    "    Tabular **Q-learning** (off-policy, 1-step TD) control.\n",
    "\n",
    "    Behavior policy: ε-greedy w.r.t. current Q (for exploration).\n",
    "    Target policy:   greedy (max over actions in s')  — this is what makes it **off-policy**.\n",
    "\n",
    "    Update (per step):\n",
    "        Q(s,a) ← Q(s,a) + α [ r + γ * max_{a'} Q(s', a') − Q(s,a) ].\n",
    "    \"\"\"\n",
    "    nS, nA = env.observation_space.n, env.action_space.n\n",
    "    Q = np.zeros((nS, nA), dtype=float)\n",
    "    visits = np.zeros((nS, nA), dtype=int)  # diagnostic: how often each (s,a) is updated\n",
    "    V_snaps = []\n",
    "\n",
    "    def eps_greedy_action(s, eps):\n",
    "        \"\"\"ε-greedy over Q[s]: with prob ε pick random action; else greedy with random tie-break.\"\"\"\n",
    "        if np.random.rand() < eps:\n",
    "            return np.random.randint(nA)\n",
    "        row = Q[s]\n",
    "        m = row.max()\n",
    "        candidates = np.flatnonzero(row == m)  # random tie-breaking argmax\n",
    "        return int(np.random.choice(candidates))\n",
    "\n",
    "    for ep in tqdm(range(episodes), desc=\"Q-learning control\"):\n",
    "        s, _ = env.reset()\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            ##########################################\n",
    "            # TODO: step the environment, update Q and choose action\n",
    "            ##########################################\n",
    "\n",
    "        # light snapshots early on (every 10 episodes up to 1k)\n",
    "        if (ep % 10) == 0 and ep <= 1000:\n",
    "            V_snaps.append(Q.max(axis=1).copy())\n",
    "\n",
    "    return Q, V_snaps\n",
    "\n",
    "\n",
    "Q, V_snaps = q_learning(env, episodes=50000, gamma=0.95, alpha=0.01, epsilon=0.01)\n",
    "V = Q.max(axis=1)\n",
    "print_values(V, 4, 12, title=\"State Values (greedy after Q-learning)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2025ocrl-pset",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
